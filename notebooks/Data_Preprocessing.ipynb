{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\mshaf\\anaconda3\\envs\\jbhi2\\lib\\site-packages (2.24.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\mshaf\\anaconda3\\envs\\jbhi2\\lib\\site-packages (from requests) (1.25.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mshaf\\anaconda3\\envs\\jbhi2\\lib\\site-packages (from requests) (2020.12.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\mshaf\\anaconda3\\envs\\jbhi2\\lib\\site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\mshaf\\anaconda3\\envs\\jbhi2\\lib\\site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: xlrd in c:\\users\\mshaf\\anaconda3\\envs\\jbhi2\\lib\\site-packages (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n",
    "!pip install xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "# common imports\n",
    "import zipfile\n",
    "import time\n",
    "# import datetime, timedelta\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "from datetime import date\n",
    "from dateutil import relativedelta\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "from io import StringIO\n",
    "import requests\n",
    "import json\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import os\n",
    "import math\n",
    "from subprocess import check_output\n",
    "from IPython.display import display\n",
    "import logging\n",
    "import yaml\n",
    "from collections import Counter\n",
    "import re\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common variables\n",
    "\n",
    "load_from_scratch = True\n",
    "\n",
    "# control whether to save dataframe with transformed data\n",
    "save_transformed_dataframe = True\n",
    "\n",
    "# control whether rows containing erroneous values are removed from the saved dataset\n",
    "remove_bad_values = True\n",
    "\n",
    "# name of file containing pickled dataframe version of input (unprocessed) dataset\n",
    "pickled_input_dataframe = \"2014_2020.pkl\"\n",
    "\n",
    "# name of file to which prepared data set is saved as a pickled dataframe\n",
    "pickled_output_dataframe = \"cleaned_Data.pkl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "streetcar_vehicles = list(range(4000,4006))+ list(range(4010,4200)) +  list(range(4200,4252)) + [4900]\n",
    "streetcar_vehicles = streetcar_vehicles + [4400] + list(range(4402,4508))\n",
    "#print(\"valid streetcars\",streetcar_vehicles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_vehicles = list(range(1000,1150))+ list(range(2000,2111)) + list(range(2150,2156)) + list(range(2240,2486))\n",
    "bus_vehicles = bus_vehicles + list(range(2600,2620)) + list(range(2700,2766)) + list(range(2767,2859))\n",
    "bus_vehicles = bus_vehicles + list(range(7000,7135)) + list(range(7400,7450)) + list(range(7500,7620)) + list(range(7620,7882))\n",
    "bus_vehicles = bus_vehicles + list(range(8000,8100)) + list(range(9000,9027))\n",
    "valid_vehicles = streetcar_vehicles + bus_vehicles\n",
    "#Using ony valid vehicles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the valid list of TTC Streetcar routes\n",
    "valid_routes = ['501','502','503','504','505','506','509','510','511','512','301','304','306','310']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original valid directions\n",
    "# valid_directions = ['E/B','W/B','N/B','S/B','B/W']\n",
    "# revised valid directions to include lowercasing and removal of '/' and simplify to single letter\n",
    "valid_directions = ['e','w','n','s','b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_days = ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the directory for that this notebook is in and return the directory containing data files\n",
    "\n",
    "def get_path():\n",
    "    rawpath = os.getcwd()\n",
    "    # data is in a directory called \"data\" that is a sibling to the directory containing the notebook\n",
    "    path = os.path.abspath(os.path.join(rawpath, '..', 'data'))\n",
    "    return(path)\n",
    "\n",
    "# given a path return the list of xls files in the directory\n",
    "def get_xls_list(path):\n",
    "    files = os.listdir(path)\n",
    "    files_xls = [f for f in files if f[-4:] == 'xlsx']\n",
    "    print(files)\n",
    "    print(files_xls)\n",
    "    return(files_xls)\n",
    "\n",
    "def load_xls(path, files_xls, firstfile, firstsheet, df):\n",
    "    for f in files_xls:\n",
    "        print(\"file name\",f)\n",
    "        xlsf = pd.ExcelFile(os.path.join(path,f))\n",
    "        # iterate through sheets\n",
    "        for sheet_name in xlsf.sheet_names:\n",
    "            print(\"sheet_name\",sheet_name)\n",
    "            if (f != firstfile) or (sheet_name != firstsheet):\n",
    "                print(\"sheet_name in loop\",sheet_name)\n",
    "                data = pd.read_excel(os.path.join(path,f),sheet_name=sheet_name)    \n",
    "                df = df.append(data)\n",
    "    return (df)\n",
    "\n",
    "# given a path and a filename, load all the XLS files in the path into a dataframe and save\n",
    "# to the dataframe to the filename\n",
    "def reloader(path,picklename):\n",
    "    # get list of all xls files in the path\n",
    "    files_xls = get_xls_list(path)\n",
    "    print(\"list of xls\",files_xls)\n",
    "    # seed initial tab on initial xls file\n",
    "    dfnew = pd.read_excel(os.path.join(path,files_xls[0]))\n",
    "    # get the list of sheets in the first file\n",
    "    xlsf = pd.ExcelFile(os.path.join(path,files_xls[0]))\n",
    "    # load the remaining tabs from all the other xls\n",
    "    # pass the first file (files_xls[0]) and the first tab in that file (xlsf[0]) explicitly\n",
    "    dflatest = load_xls(path,files_xls,files_xls[0],xlsf.sheet_names[0], dfnew)\n",
    "    # save dataframe to pickle\n",
    "    dflatest.to_pickle(os.path.join(path,picklename))\n",
    "    # return dataframe loaded with all tabs of all xls files\n",
    "    return(dflatest)\n",
    "    \n",
    "    # define categories for input columns\n",
    "def define_feature_categories(df):\n",
    "    allcols = list(df)\n",
    "    print(\"all cols\",allcols)\n",
    "    textcols = ['Incident','Location'] # \n",
    "    continuouscols = ['Min Delay','Min Gap'] \n",
    "                      # columns to deal with as continuous values - no embeddings\n",
    "    timecols = ['Report Date','Time']\n",
    "    collist = ['Day','Vehicle','Route','Direction']\n",
    "    for col in continuouscols:\n",
    "        df[col] = df[col].astype(float)\n",
    "    print('texcols: ',textcols)\n",
    "    print('continuouscols: ',continuouscols)\n",
    "    print('timecols: ',timecols)\n",
    "    print('collist: ',collist)\n",
    "    return(allcols,textcols,continuouscols,timecols,collist)\n",
    "# fill missing values according to the column category\n",
    "def fill_missing(dataset,allcols,textcols,continuouscols,timecols,collist):\n",
    "    logging.debug(\"before mv\")\n",
    "    for col in collist:\n",
    "        dataset[col].fillna(value=\"missing\", inplace=True)\n",
    "    for col in continuouscols:\n",
    "        dataset[col].fillna(value=0.0,inplace=True)\n",
    "    for col in textcols:\n",
    "        dataset[col].fillna(value=\"missing\", inplace=True)\n",
    "    return (dataset)\n",
    "# read in data, either from original XLS files in data directory or from pickled dataframe containing\n",
    "def ingest_data(path):\n",
    "    if load_from_scratch:\n",
    "        unpickled_df = reloader(path,pickled_input_dataframe)\n",
    "        logging.debug(\"reloader done\")\n",
    "    else:\n",
    "        unpickled_df = pd.read_pickle(os.path.join(path,pickled_input_dataframe))\n",
    "    return(unpickled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General cleanup\n",
    "- correct types for Route and Vehicle\n",
    "- fill missing values\n",
    "- create report-date-time index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dataset incorporated some anomalies in the 2019 data, including:\n",
    "# extraneous Incident ID in April 2019 tab\n",
    "# Gap and Delay columns in April and June 2019 tabs for what had otherwise been called Min Gap and Min Delay\n",
    "# this function cleans up these anomalies\n",
    "def fix_anomalous_columns(df):\n",
    "    # for rows where there is NaN in the Min Delay or Min Gap columns, copy over value from Delay or Gap\n",
    "    # df.Temp_Rating.fillna(df.Farheit, inplace=True)\n",
    "    df['Min Delay'].fillna(df['Delay'], inplace=True)\n",
    "    df['Min Gap'].fillna(df['Gap'], inplace=True)\n",
    "    # now that the useful values have been copied from Delay and Gap, remove them\n",
    "    del df['Delay']\n",
    "    del df['Gap']\n",
    "    # remove Incident ID column - it's extraneous\n",
    "    del df['Incident ID']\n",
    "    return(df)\n",
    "\n",
    "def replace_time(date_time_value,time_value):\n",
    "    ''' given a datetime replace the time portion '''\n",
    "     \n",
    "    date_time_value = date_time_value.replace(hour=time_value.hour,minute=time_value.minute,second=time_value.minute)\n",
    "    return(date_time_value)\n",
    "\n",
    "def general_cleanup(df):\n",
    "    # ensure Route and Vehicle are strings, not numeric\n",
    "    df['Route'] = df['Route'].astype(str)\n",
    "    df['Vehicle'] = df['Vehicle'].astype(str)\n",
    "    # remove extraneous characters left from Vehicle values being floats\n",
    "    df['Vehicle'] = df['Vehicle'].str[:-2]\n",
    "    # tactical definition of categories\n",
    "    allcols,textcols,continuouscols,timecols,collist = define_feature_categories(df)\n",
    "    # fill in missing values\n",
    "    df.isnull().sum(axis = 0)\n",
    "    df = fix_anomalous_columns(df)\n",
    "    df = fill_missing(df,allcols,textcols,continuouscols,timecols,collist)\n",
    "    # create new column combining date + time (needed for resampling) and make it the index\n",
    "    df['Report Date Time'] = df.apply(lambda x: replace_time(x['Report Date'], x['Time']), axis=1)\n",
    "    df.index = df['Report Date Time']\n",
    "    # return the updated dataframe along with the column category lists\n",
    "    return(df,allcols,textcols,continuouscols,timecols,collist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Route, Vehicle, Direction and Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------\n",
    "#Functions for cleaning the route columns\n",
    "def check_route (x):\n",
    "    if x in valid_routes:\n",
    "        return(x)\n",
    "    else:\n",
    "        return(\"bad route\")\n",
    "def route_cleanup(df):\n",
    "    print(\"Route count pre cleanup\",df['Route'].nunique())\n",
    "    # df['Route'].value_counts()\n",
    "    # replace bad route with common token\n",
    "    df['Route'] = df['Route'].apply(lambda x:check_route(x))\n",
    "    print(\"route count post cleanup\",df['Route'].nunique())\n",
    "    return(df)    \n",
    "\n",
    "\n",
    "#-------------------------------------------------------\n",
    "#Functions for checking and cleaning the vehicle columns\n",
    "def check_vehicle (x):\n",
    "    if str.isdigit(x):\n",
    "        if int(x) in valid_vehicles:\n",
    "            return x\n",
    "        else:\n",
    "            return(\"bad vehicle\")\n",
    "    else:\n",
    "        return(\"bad vehicle\")\n",
    "    \n",
    "def vehicle_cleanup(df):\n",
    "    print(\"Vehicle count pre cleanup\",df['Vehicle'].nunique())\n",
    "    df['Vehicle'] = df['Vehicle'].apply(lambda x:check_vehicle(x))\n",
    "    print(\"Vehicle count post cleanup\",df['Vehicle'].nunique())\n",
    "    return(df)\n",
    "\n",
    "\n",
    "#-------------------------------------------------------\n",
    "#Functions for cleaning the direction columns\n",
    "def check_direction (x):\n",
    "    if x in valid_directions:\n",
    "        return(x)\n",
    "    else:\n",
    "        return(\"bad direction\")\n",
    "    \n",
    "def direction_cleanup(df):\n",
    "    print(\"Direction count pre cleanup\",df['Direction'].nunique())\n",
    "    df['Direction'] = df['Direction'].str.lower()\n",
    "    df['Direction'] = df['Direction'].str.replace('/','')\n",
    "    df['Direction'] = df['Direction'].replace({'eastbound':'e','westbound':'w','southbound':'s','northbound':'n'})\n",
    "    df['Direction'] = df['Direction'].replace('b','',regex=True)\n",
    "    df['Direction'] = df['Direction'].apply(lambda x:check_direction(x))\n",
    "    print(\"Direction count post cleanup\",df['Direction'].nunique())\n",
    "    return(df)\n",
    "\n",
    "\n",
    "#-------------------------------------------------------\n",
    "#Functions for cleaning the Location columns\n",
    "def clean_conjunction(intersection):\n",
    "    intersection = re.sub(\" *& *\",\" and \",intersection)\n",
    "    intersection = re.sub(\" */ *\",\" and \",intersection)\n",
    "    return(intersection)\n",
    "\n",
    "def order_location(intersection):\n",
    "    # for any string with the format \"* and *\" if the value before the and is alphabetically\n",
    "    # higher than the value after the and, swap the values\n",
    "    conj = \" and \"\n",
    "    alpha_ordered_intersection = intersection\n",
    "    if conj in intersection:\n",
    "        end_first_street = intersection.find(conj)\n",
    "        if (end_first_street > 0) and (len(intersection) > (end_first_street + len(conj))):\n",
    "            start_second_street = intersection.find(conj) + len(conj)\n",
    "            first_street = intersection[0:end_first_street]\n",
    "            second_street = intersection[start_second_street:]\n",
    "            alpha_ordered_intersection = min(first_street,second_street)+conj+max(first_street,second_street)\n",
    "    return(alpha_ordered_intersection)\n",
    "\n",
    "def location_cleanup(df):\n",
    "    print(\"Location count pre cleanup\",df['Location'].nunique())\n",
    "    # make all location values lower case\n",
    "    df['Location'] = df['Location'].str.lower()\n",
    "    # make substitutions to eliminate obvious duplicate tokens\n",
    "    df['Location'] = df['Location'].replace({'broadviewstation':'broadview station',' at ':' and ',' stn':' station',' ave.':'','/':' and ','roncy':'roncesvalles','carhouse':'yard','yard.':'yard','st. clair':'st clair','ronc. ':'roncesvalles ','long branch':'longbranch','garage':'yard','barns':'yard',' & ':' and '}, regex=True)\n",
    "    # put intersection values into consistent order\n",
    "    df['Location'] = df['Location'].apply(lambda x:order_location(x))\n",
    "    print(\"Location count post cleanup\",df['Location'].nunique())\n",
    "    return(df)\n",
    "\n",
    "#-------------------------------------------------------\n",
    "# remove rows with bad values\n",
    "def remove_bad(df):\n",
    "    df = df[df.Vehicle != 'bad vehicle']\n",
    "    df = df[df.Direction != 'bad direction']\n",
    "    df = df[df.Route != 'bad route']\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2014_2020.pkl', 'cleaned_Data.pkl', 'excessdata', 'routedirection.csv', 'routedirection_badvalues.csv', 'tensorboard_log', 'ttc-streetcar-delay-data-2014.xlsx', 'ttc-streetcar-delay-data-2015.xlsx', 'ttc-streetcar-delay-data-2016.xlsx', 'ttc-streetcar-delay-data-2017.xlsx', 'ttc-streetcar-delay-data-2018.xlsx', 'ttc-streetcar-delay-data-2019.xlsx', 'ttc-streetcar-delay-data-2020.xlsx']\n",
      "['ttc-streetcar-delay-data-2014.xlsx', 'ttc-streetcar-delay-data-2015.xlsx', 'ttc-streetcar-delay-data-2016.xlsx', 'ttc-streetcar-delay-data-2017.xlsx', 'ttc-streetcar-delay-data-2018.xlsx', 'ttc-streetcar-delay-data-2019.xlsx', 'ttc-streetcar-delay-data-2020.xlsx']\n",
      "list of xls ['ttc-streetcar-delay-data-2014.xlsx', 'ttc-streetcar-delay-data-2015.xlsx', 'ttc-streetcar-delay-data-2016.xlsx', 'ttc-streetcar-delay-data-2017.xlsx', 'ttc-streetcar-delay-data-2018.xlsx', 'ttc-streetcar-delay-data-2019.xlsx', 'ttc-streetcar-delay-data-2020.xlsx']\n",
      "file name ttc-streetcar-delay-data-2014.xlsx\n",
      "sheet_name Jan 2014\n",
      "sheet_name Feb 2014\n",
      "sheet_name in loop Feb 2014\n",
      "sheet_name Mar 2014\n",
      "sheet_name in loop Mar 2014\n",
      "sheet_name Apr 2014\n",
      "sheet_name in loop Apr 2014\n",
      "sheet_name May 2014\n",
      "sheet_name in loop May 2014\n",
      "sheet_name Jun 2014\n",
      "sheet_name in loop Jun 2014\n",
      "sheet_name July 2014\n",
      "sheet_name in loop July 2014\n",
      "sheet_name Aug 2014\n",
      "sheet_name in loop Aug 2014\n",
      "sheet_name Sept 2014\n",
      "sheet_name in loop Sept 2014\n",
      "sheet_name Oct 2014\n",
      "sheet_name in loop Oct 2014\n",
      "sheet_name Nov 2014\n",
      "sheet_name in loop Nov 2014\n",
      "sheet_name Dec 2014\n",
      "sheet_name in loop Dec 2014\n",
      "file name ttc-streetcar-delay-data-2015.xlsx\n",
      "sheet_name Jan 2015\n",
      "sheet_name in loop Jan 2015\n",
      "sheet_name Feb 2015\n",
      "sheet_name in loop Feb 2015\n",
      "sheet_name Mar 2015\n",
      "sheet_name in loop Mar 2015\n",
      "sheet_name Apr 2015\n",
      "sheet_name in loop Apr 2015\n",
      "sheet_name May 2015\n",
      "sheet_name in loop May 2015\n",
      "sheet_name Jun 2015\n",
      "sheet_name in loop Jun 2015\n",
      "sheet_name July 2015\n",
      "sheet_name in loop July 2015\n",
      "sheet_name Aug 2015\n",
      "sheet_name in loop Aug 2015\n",
      "sheet_name Sept 2015\n",
      "sheet_name in loop Sept 2015\n",
      "sheet_name Oct 2015\n",
      "sheet_name in loop Oct 2015\n",
      "sheet_name Nov 2015\n",
      "sheet_name in loop Nov 2015\n",
      "sheet_name Dec 2015\n",
      "sheet_name in loop Dec 2015\n",
      "file name ttc-streetcar-delay-data-2016.xlsx\n",
      "sheet_name Jan 2016\n",
      "sheet_name in loop Jan 2016\n",
      "sheet_name Feb 2016\n",
      "sheet_name in loop Feb 2016\n",
      "sheet_name Mar 2016\n",
      "sheet_name in loop Mar 2016\n",
      "sheet_name Apr 2016\n",
      "sheet_name in loop Apr 2016\n",
      "sheet_name May 2016\n",
      "sheet_name in loop May 2016\n",
      "sheet_name Jun 2016\n",
      "sheet_name in loop Jun 2016\n",
      "sheet_name July 2016\n",
      "sheet_name in loop July 2016\n",
      "sheet_name Aug 2016\n",
      "sheet_name in loop Aug 2016\n",
      "sheet_name Sept 2016\n",
      "sheet_name in loop Sept 2016\n",
      "sheet_name Oct 2016\n",
      "sheet_name in loop Oct 2016\n",
      "sheet_name Nov 2016\n",
      "sheet_name in loop Nov 2016\n",
      "sheet_name Dec 2016\n",
      "sheet_name in loop Dec 2016\n",
      "file name ttc-streetcar-delay-data-2017.xlsx\n",
      "sheet_name Jan 2017\n",
      "sheet_name in loop Jan 2017\n",
      "sheet_name Feb 2017\n",
      "sheet_name in loop Feb 2017\n",
      "sheet_name Mar 2017\n",
      "sheet_name in loop Mar 2017\n",
      "sheet_name Apr 2017\n",
      "sheet_name in loop Apr 2017\n",
      "sheet_name May 2017\n",
      "sheet_name in loop May 2017\n",
      "sheet_name Jun 2017\n",
      "sheet_name in loop Jun 2017\n",
      "sheet_name July 2017\n",
      "sheet_name in loop July 2017\n",
      "sheet_name Aug 2017\n",
      "sheet_name in loop Aug 2017\n",
      "sheet_name Sept 2017 \n",
      "sheet_name in loop Sept 2017 \n",
      "sheet_name Oct 2017\n",
      "sheet_name in loop Oct 2017\n",
      "sheet_name Nov 2017\n",
      "sheet_name in loop Nov 2017\n",
      "sheet_name Dec 2017\n",
      "sheet_name in loop Dec 2017\n",
      "file name ttc-streetcar-delay-data-2018.xlsx\n",
      "sheet_name Jan 2018\n",
      "sheet_name in loop Jan 2018\n",
      "sheet_name Feb 2018 \n",
      "sheet_name in loop Feb 2018 \n",
      "sheet_name Mar 2018\n",
      "sheet_name in loop Mar 2018\n",
      "sheet_name Apr 2018 \n",
      "sheet_name in loop Apr 2018 \n",
      "sheet_name May 2018 \n",
      "sheet_name in loop May 2018 \n",
      "sheet_name June 2018 \n",
      "sheet_name in loop June 2018 \n",
      "sheet_name July 2018\n",
      "sheet_name in loop July 2018\n",
      "sheet_name Aug 2018 \n",
      "sheet_name in loop Aug 2018 \n",
      "sheet_name Sept 2018 \n",
      "sheet_name in loop Sept 2018 \n",
      "sheet_name Oct 2018 \n",
      "sheet_name in loop Oct 2018 \n",
      "sheet_name Nov 2018 \n",
      "sheet_name in loop Nov 2018 \n",
      "sheet_name Dec 2018 \n",
      "sheet_name in loop Dec 2018 \n",
      "file name ttc-streetcar-delay-data-2019.xlsx\n",
      "sheet_name Jan 2019 \n",
      "sheet_name in loop Jan 2019 \n",
      "sheet_name Feb 2019 \n",
      "sheet_name in loop Feb 2019 \n",
      "sheet_name Mar 2019 \n",
      "sheet_name in loop Mar 2019 \n",
      "sheet_name Apr 2019\n",
      "sheet_name in loop Apr 2019\n",
      "sheet_name May 2019 \n",
      "sheet_name in loop May 2019 \n",
      "sheet_name June 2019 \n",
      "sheet_name in loop June 2019 \n",
      "sheet_name July 2019 \n",
      "sheet_name in loop July 2019 \n",
      "sheet_name Aug, 2019 \n",
      "sheet_name in loop Aug, 2019 \n",
      "sheet_name Sept 2019 \n",
      "sheet_name in loop Sept 2019 \n",
      "sheet_name Oct 2019 \n",
      "sheet_name in loop Oct 2019 \n",
      "sheet_name  Nov 2019\n",
      "sheet_name in loop  Nov 2019\n",
      "sheet_name Dec 2019\n",
      "sheet_name in loop Dec 2019\n",
      "file name ttc-streetcar-delay-data-2020.xlsx\n",
      "sheet_name Jan 2020\n",
      "sheet_name in loop Jan 2020\n",
      "sheet_name Feb 2020\n",
      "sheet_name in loop Feb 2020\n",
      "sheet_name March 2020\n",
      "sheet_name in loop March 2020\n",
      "sheet_name April 2020\n",
      "sheet_name in loop April 2020\n",
      "sheet_name May 2020\n",
      "sheet_name in loop May 2020\n",
      "sheet_name June 2020\n",
      "sheet_name in loop June 2020\n",
      "sheet_name July 2020\n",
      "sheet_name in loop July 2020\n",
      "sheet_name August 2020\n",
      "sheet_name in loop August 2020\n",
      "all cols ['Report Date', 'Route', 'Time', 'Day', 'Location', 'Incident', 'Min Delay', 'Min Gap', 'Direction', 'Vehicle', 'Incident ID', 'Delay', 'Gap']\n",
      "texcols:  ['Incident', 'Location']\n",
      "continuouscols:  ['Min Delay', 'Min Gap']\n",
      "timecols:  ['Report Date', 'Time']\n",
      "collist:  ['Day', 'Vehicle', 'Route', 'Direction']\n"
     ]
    }
   ],
   "source": [
    "# master cell to call the other functions\n",
    "# get the path for data files\n",
    "path = get_path()\n",
    "# load route direction and delay data datframes\n",
    "df = ingest_data(path)\n",
    "df,allcols,textcols,continuouscols,timecols,collist = general_cleanup(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "record count by year pre processing:  Counter({2018: 15612, 2016: 14021, 2017: 13762, 2015: 12221, 2019: 11882, 2014: 11027, 2020: 4177})\n",
      "Route count pre cleanup 124\n",
      "route count post cleanup 15\n",
      "Vehicle count pre cleanup 2727\n",
      "Vehicle count post cleanup 1020\n",
      "Direction count pre cleanup 103\n",
      "Direction count post cleanup 5\n",
      "Location count pre cleanup 17679\n",
      "Location count post cleanup 11355\n",
      "Bad route count pre: 2618\n",
      "Bad direction count pre: 414\n",
      "Bad vehicle count pre: 16975\n",
      "Bad route count: 0\n",
      "Bad direction count: 0\n",
      "Bad vehicle count: 0\n",
      "df.shape output post removal of bad records  (63390, 11)\n",
      "path is  C:\\Users\\mshaf\\MachineLearningTTC\\data\n",
      "file_name is  C:\\Users\\mshaf\\MachineLearningTTC\\data\\cleaned_Data.pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Report Date</th>\n",
       "      <th>Route</th>\n",
       "      <th>Time</th>\n",
       "      <th>Day</th>\n",
       "      <th>Location</th>\n",
       "      <th>Incident</th>\n",
       "      <th>Min Delay</th>\n",
       "      <th>Min Gap</th>\n",
       "      <th>Direction</th>\n",
       "      <th>Vehicle</th>\n",
       "      <th>Report Date Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Report Date Time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-01-02 06:31:31</th>\n",
       "      <td>2014-01-02</td>\n",
       "      <td>505</td>\n",
       "      <td>06:31:00</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>dundas and roncesvalles</td>\n",
       "      <td>Late Leaving Garage</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>e</td>\n",
       "      <td>4018</td>\n",
       "      <td>2014-01-02 06:31:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-02 12:43:43</th>\n",
       "      <td>2014-01-02</td>\n",
       "      <td>504</td>\n",
       "      <td>12:43:00</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>king and shaw</td>\n",
       "      <td>Utilized Off Route</td>\n",
       "      <td>20.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>e</td>\n",
       "      <td>4128</td>\n",
       "      <td>2014-01-02 12:43:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-02 14:01:01</th>\n",
       "      <td>2014-01-02</td>\n",
       "      <td>501</td>\n",
       "      <td>14:01:00</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>bingham and kingston road</td>\n",
       "      <td>Held By</td>\n",
       "      <td>13.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>w</td>\n",
       "      <td>4016</td>\n",
       "      <td>2014-01-02 14:01:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-02 14:22:22</th>\n",
       "      <td>2014-01-02</td>\n",
       "      <td>504</td>\n",
       "      <td>14:22:00</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>king st. and roncesvalles</td>\n",
       "      <td>Investigation</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>w</td>\n",
       "      <td>4175</td>\n",
       "      <td>2014-01-02 14:22:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-02 16:42:42</th>\n",
       "      <td>2014-01-02</td>\n",
       "      <td>504</td>\n",
       "      <td>16:42:00</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>bathurst and king</td>\n",
       "      <td>Utilized Off Route</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>e</td>\n",
       "      <td>4080</td>\n",
       "      <td>2014-01-02 16:42:42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Report Date Route      Time       Day  \\\n",
       "Report Date Time                                            \n",
       "2014-01-02 06:31:31  2014-01-02   505  06:31:00  Thursday   \n",
       "2014-01-02 12:43:43  2014-01-02   504  12:43:00  Thursday   \n",
       "2014-01-02 14:01:01  2014-01-02   501  14:01:00  Thursday   \n",
       "2014-01-02 14:22:22  2014-01-02   504  14:22:00  Thursday   \n",
       "2014-01-02 16:42:42  2014-01-02   504  16:42:00  Thursday   \n",
       "\n",
       "                                      Location             Incident  \\\n",
       "Report Date Time                                                      \n",
       "2014-01-02 06:31:31    dundas and roncesvalles  Late Leaving Garage   \n",
       "2014-01-02 12:43:43              king and shaw   Utilized Off Route   \n",
       "2014-01-02 14:01:01  bingham and kingston road              Held By   \n",
       "2014-01-02 14:22:22  king st. and roncesvalles        Investigation   \n",
       "2014-01-02 16:42:42          bathurst and king   Utilized Off Route   \n",
       "\n",
       "                     Min Delay  Min Gap Direction Vehicle    Report Date Time  \n",
       "Report Date Time                                                               \n",
       "2014-01-02 06:31:31        4.0      8.0         e    4018 2014-01-02 06:31:31  \n",
       "2014-01-02 12:43:43       20.0     22.0         e    4128 2014-01-02 12:43:43  \n",
       "2014-01-02 14:01:01       13.0     19.0         w    4016 2014-01-02 14:01:01  \n",
       "2014-01-02 14:22:22        7.0     11.0         w    4175 2014-01-02 14:22:22  \n",
       "2014-01-02 16:42:42        3.0      6.0         e    4080 2014-01-02 16:42:42  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# get record count by year\n",
    "from collections import Counter\n",
    "df_year = pd.DatetimeIndex(df['Report Date Time']).year\n",
    "print(\"record count by year pre processing: \", str(Counter(df_year)))\n",
    "\n",
    "# check that the values for April 2019 are correct as there were anomolaies in this section\n",
    "df[df['Report Date Time'].astype(str).str[:7]=='2019-04']\n",
    "\n",
    "# cleanup Route\n",
    "logging.debug(\"df.shape output pre route cleanup\",df.shape)\n",
    "df = route_cleanup(df) \n",
    "df = vehicle_cleanup(df)\n",
    "df = direction_cleanup(df)\n",
    "df = location_cleanup(df)\n",
    "logging.debug(\"df.shape output post location\",df.shape)\n",
    "print(\"Bad route count pre:\",df[df.Route == 'bad route'].shape[0])\n",
    "print(\"Bad direction count pre:\",df[df.Direction == 'bad direction'].shape[0])\n",
    "print(\"Bad vehicle count pre:\",df[df.Vehicle == 'bad vehicle'].shape[0])\n",
    "if remove_bad_values:\n",
    "    df = remove_bad(df)\n",
    "print(\"Bad route count:\",df[df.Route == 'bad route'].shape[0])\n",
    "print(\"Bad direction count:\",df[df.Direction == 'bad direction'].shape[0])\n",
    "print(\"Bad vehicle count:\",df[df.Vehicle == 'bad vehicle'].shape[0])\n",
    "\n",
    "\n",
    "# pickle the cleansed dataframe in the file named cleaned_Data.pkl\n",
    "print(\"df.shape output post removal of bad records \",df.shape)\n",
    "if save_transformed_dataframe:\n",
    "    print(\"path is \",path)\n",
    "    file_name = os.path.join(path,pickled_output_dataframe)\n",
    "    print(\"file_name is \",file_name)\n",
    "    df.to_pickle(file_name)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({2014: 9371,\n",
       "         2015: 10898,\n",
       "         2016: 11908,\n",
       "         2017: 9891,\n",
       "         2018: 12011,\n",
       "         2019: 7474,\n",
       "         2020: 1837})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get record count by year\n",
    "\n",
    "df_year = pd.DatetimeIndex(df['Report Date Time']).year\n",
    "Counter(df_year)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
